'''import logging
import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.datasets import Dataset

S3_DATASET = Dataset("s3://data-stack/raw/earthquake")

OWNER = "15683"
DAG_ID = "raw_from_api_to_s3"

LAYER = "raw"
SOURCE = "earthquake"

LONG_DESCRIPTION = """
# LONG DESCRIPTION
"""

SHORT_DESCRIPTION = "SHORT DESCRIPTION"

args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 5, 1, tz="Europe/Moscow"),
    "catchup": False,
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}

def get_dates(**context) -> tuple[str, str]:
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")
    return start_date, end_date

def get_and_transfer_api_data_to_s3(**context):
    try:
        access_key = Variable.get("access_key")
        secret_key = Variable.get("secret_key")
    except KeyError:
        logging.error("Variables 'access_key' or 'secret_key' not found in Airflow Admin!")
        raise

    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for dates: {start_date}/{end_date}")

    con = duckdb.connect()

    try:
        con.sql("INSTALL httpfs; LOAD httpfs;")

        query = f"""
        SET TIMEZONE='UTC';
        SET s3_url_style = 'path';
        SET s3_endpoint = 'minio:9000';
        SET s3_access_key_id = '{access_key}';
        SET s3_secret_access_key = '{secret_key}';
        SET s3_use_ssl = FALSE;
                            
        COPY
        (
            SELECT *
            FROM read_csv_auto('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime={start_date}&endtime={end_date}') 
        ) 
        TO 's3://data-stack/{LAYER}/{SOURCE}/{start_date}/{start_date}_data.parquet'
        (FORMAT 'PARQUET', CODEC 'GZIP');
        """

        logging.info("Executing DuckDB query...")
        con.sql(query)
        logging.info(f"âœ… Download for date success: {start_date}")

    except Exception as e:
        logging.error(f"DuckDB Error: {e}")
        raise
    finally:
        con.close()

with DAG(
        dag_id="raw_from_api_to_s3",
        schedule_interval="@daily",
        default_args=args,
        tags=["s3", "raw"],
        description=SHORT_DESCRIPTION,
        max_active_runs=1,
        catchup=False
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(task_id="start")

    task_transfer = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,
        outlets=[S3_DATASET]
    )

    end = EmptyOperator(task_id="end")

    start >> task_transfer >> end'''


import logging
import pendulum
import pandas as pd
import requests

from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.datasets import Dataset

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Dataset, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±ÑƒĞ´ĞµÑ‚ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ DAG
S3_FOOTBALL_DATASET = Dataset("s3://data-stack/raw/football")

OWNER = "15683"
SOURCE = "football-data.org"
COMPETITION = "CL"  # Champions League


def get_and_transfer_api_data_to_s3(**context):
    """
    ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ¼Ğ°Ñ‚Ñ‡Ğ°Ñ… Ğ¸Ğ· football-data.org API Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ² MinIO.
    """
    try:
        api_key = Variable.get("football_api_key")
        s3_access_key = Variable.get("access_key")
        s3_secret_key = Variable.get("secret_key")
    except KeyError:
        logging.error("ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ football_api_key, access_key, Ğ¸Ğ»Ğ¸ secret_key!")
        raise

    data_interval_start = context["data_interval_start"]
    date_str = data_interval_start.format("YYYY-MM-DD")

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ URL Ğ¸ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ API
    url = f"https://api.football-data.org/v4/competitions/{COMPETITION}/matches"
    headers = {"X-Auth-Token": api_key}

    logging.info(f"ğŸ’» Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· API Ğ´Ğ»Ñ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {COMPETITION}")

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ HTTP (4xx, 5xx)
        data = response.json()
        matches = data.get("matches", [])

        if not matches:
            logging.warning("API Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ» Ğ¼Ğ°Ñ‚Ñ‡ĞµĞ¹. ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº.")
            return

        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Pandas Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ JSON
        df = pd.json_normalize(
            matches,
            sep='_'  # Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ score_fullTime_home
        )

        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² Parquet
        s3_path = f"s3://data-stack/raw/football/{date_str}/{COMPETITION}_matches.parquet"
        storage_options = {
            "key": s3_access_key,
            "secret": s3_secret_key,
            "endpoint_url": "http://minio:9000",
            "client_kwargs": {"use_ssl": False}
        }

        logging.info(f"Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ {len(df)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² {s3_path}")
        df.to_parquet(s3_path, index=False, storage_options=storage_options)
        logging.info("âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² S3.")

    except requests.exceptions.RequestException as e:
        logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğº API: {e}")
        raise
    except Exception as e:
        logging.error(f"ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")
        raise


with DAG(
        dag_id="raw_football_matches_from_api_to_s3",
        schedule_interval="@daily",
        start_date=pendulum.datetime(2025, 12, 1, tz="Europe/Moscow"),
        default_args={"owner": OWNER},
        tags=["s3", "raw", "football"],
        description="Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ‡Ğ°Ñ… Ğ¸Ğ· API Ğ² S3",
        max_active_runs=1,
        catchup=False,
) as dag:
    start = EmptyOperator(task_id="start")

    task_transfer = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,
        outlets=[S3_FOOTBALL_DATASET],  # ĞŸÑƒĞ±Ğ»Ğ¸ĞºÑƒĞµĞ¼ Dataset Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ DAG-Ğ°
    )

    end = EmptyOperator(task_id="end")

    start >> task_transfer >> end
