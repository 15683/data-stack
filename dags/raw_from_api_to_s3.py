'''import logging
import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG
OWNER = "15683"
DAG_ID = "raw_from_api_to_s3"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² DAG
LAYER = "raw"
SOURCE = "earthquake" #TODO Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾ĞµĞ³Ğ¾ API

# S3
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

LONG_DESCRIPTION = """
# LONG DESCRIPTION
"""

SHORT_DESCRIPTION = "SHORT DESCRIPTION"

args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 5, 1, tz="Europe/Moscow"),
    "catchup": True,
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}


def get_dates(**context) -> tuple[str, str]:
    """"""
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")

    return start_date, end_date


def get_and_transfer_api_data_to_s3(**context):
    """"""

    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for dates: {start_date}/{end_date}")
    con = duckdb.connect()

    con.sql(
        f"""
        SET TIMEZONE='UTC';
        INSTALL httpfs;
        LOAD httpfs;
        SET s3_url_style = 'path';
        SET s3_endpoint = 'minio:9000';
        SET s3_access_key_id = '{ACCESS_KEY}';
        SET s3_secret_access_key = '{SECRET_KEY}';
        SET s3_use_ssl = FALSE;

        COPY
        (
            SELECT
                *
            FROM              
                read_csv_auto('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime={start_date}&endtime={end_date}') AS res
        ) TO 's3://data-stack/{LAYER}/{SOURCE}/{start_date}/{start_date}_00-00-00.gz.parquet';
        """,      #TODO Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ²Ñ‹ÑˆĞµ Ğ°Ğ´Ñ€ĞµÑ Ğ½Ğ° Ğ¼Ğ¾Ğ¹ Ğ°Ğ´Ñ€ĞµÑ API
    )

    con.close()
    logging.info(f"âœ… Download for date success: {start_date}")


with DAG(
    dag_id=DAG_ID,
    schedule_interval="0 5 * * *",
    default_args=args,
    tags=["s3", "raw"],
    description=SHORT_DESCRIPTION,
    concurrency=1,
    max_active_tasks=1,
    max_active_runs=1,
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(
        task_id="start",
    )

    get_and_transfer_api_data_to_s3 = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> get_and_transfer_api_data_to_s3 >> end'''

import logging
import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG
OWNER = "15683"
DAG_ID = "raw_from_api_to_s3"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² DAG
LAYER = "raw"
SOURCE = "earthquake"

LONG_DESCRIPTION = """
# LONG DESCRIPTION
"""

SHORT_DESCRIPTION = "SHORT DESCRIPTION"

args = {
    "owner": OWNER,
    # Ğ£ĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ´Ğ°Ñ‚Ñƒ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ñ‡ĞµÑ€Ğ°ÑˆĞ½Ğ¸Ğ¼ Ğ´Ğ½ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ‚ĞµÑÑ‚ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» ÑÑ€Ğ°Ğ·Ñƒ
    "start_date": pendulum.now("Europe/Moscow").subtract(days=1),
    "catchup": False,  # ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ°
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}


def get_dates(**context) -> tuple[str, str]:
    """"""
    # Ğ‘ĞµÑ€ĞµĞ¼ Ğ´Ğ°Ñ‚Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Airflow
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")
    return start_date, end_date


def get_and_transfer_api_data_to_s3(**context):
    """"""
    # 1. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ’ĞĞ£Ğ¢Ğ Ğ˜ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ (ÑÑ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾!)
    # Ğ•ÑĞ»Ğ¸ Ğ¸Ñ… Ğ½ĞµÑ‚, Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑƒĞ¿Ğ°Ğ´ĞµÑ‚, Ğ½Ğ¾ ÑĞ°Ğ¼ DAG Ğ½Ğµ ÑĞ»Ğ¾Ğ¼Ğ°ĞµÑ‚ÑÑ
    try:
        access_key = Variable.get("access_key")
        secret_key = Variable.get("secret_key")
    except KeyError:
        logging.error("Variables 'access_key' or 'secret_key' not found in Airflow Admin!")
        raise

    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for dates: {start_date}/{end_date}")

    # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº DuckDB (in-memory)
    con = duckdb.connect()

    # 2. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    try:
        con.sql("INSTALL httpfs; LOAD httpfs;")

        query = f"""
        SET TIMEZONE='UTC';
        SET s3_url_style = 'path';
        SET s3_endpoint = 'minio:9000';
        SET s3_access_key_id = '{access_key}';
        SET s3_secret_access_key = '{secret_key}';
        SET s3_use_ssl = FALSE;

        -- Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² s3://bucket/raw/earthquake/YYYY-MM-DD/...
        COPY
        (
            SELECT *
            FROM read_csv_auto('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime={start_date}&endtime={end_date}')
        ) 
        TO 's3://data-stack/{LAYER}/{SOURCE}/{start_date}/{start_date}_data.parquet'
        (FORMAT 'PARQUET', CODEC 'GZIP');
        """
        logging.info("Executing DuckDB query...")
        con.sql(query)
        logging.info(f"âœ… Download for date success: {start_date}")

    except Exception as e:
        logging.error(f"DuckDB Error: {e}")
        raise
    finally:
        con.close()


with DAG(
        dag_id=DAG_ID,
        schedule_interval="@daily",  # Ğ—Ğ°Ğ¿ÑƒÑĞº Ñ€Ğ°Ğ· Ğ² Ğ´ĞµĞ½ÑŒ
        default_args=args,
        tags=["s3", "raw"],
        description=SHORT_DESCRIPTION,
        max_active_runs=1,
        catchup=False  # Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(task_id="start")

    task_transfer = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,
    )

    end = EmptyOperator(task_id="end")

    start >> task_transfer >> end